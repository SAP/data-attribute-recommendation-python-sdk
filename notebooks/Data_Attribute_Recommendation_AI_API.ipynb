{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9e756a78874ae8",
   "metadata": {},
   "source": [
    "## Consume Data Attribute Recommendation via AI API \n",
    "\n",
    "Deep dive into the Python SDK for the Data Attribute Recommendation service using the AI API from SAP AI Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81c7f180516fea",
   "metadata": {},
   "source": [
    "## Business Scenario\n",
    "\n",
    "Let's examine a business scenario involving product master data. Creating and maintaining product master data requires manually choosing the right categories from a pre-set list for each product.\n",
    "In this notebook, we will explore how to automate this tedious manual task using the Data Attribute Recommendation service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d16f16c5120c7",
   "metadata": {},
   "source": [
    "This example will cover:\n",
    "    \n",
    "* Data Upload\n",
    "* Model Training and Deployment\n",
    "* Inference Requests\n",
    "    \n",
    "We will work through a basic example of how to achieve these tasks using the AI API Client of the [Python SDK for Data Attribute Recommendation](https://github.com/SAP/data-attribute-recommendation-python-sdk).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c58330277823",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Exercise 01.1](#Exercise-01.1) - Installing the SDK and preparing the service key\n",
    "* [Exercise 01.2](#Exercise-01.2) - Uploading data via DAR AI API\n",
    "* [Exercise 01.3](#Exercise-01.3) - Training the model via DAR AI API\n",
    "* [Exercise 01.4](#Exercise-01.4) - Deploying the model via DAR AI API\n",
    "* [Exercise 01.5](#Exercise-01.5) - Predicting labels via DAR Inference Client\n",
    "* [Cleaning up a service instance](#Cleaning-up-a-service-instance) - Clean up all resources on the service instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe34c8f52d1566",
   "metadata": {},
   "source": [
    "# Exercise 01.1\n",
    "\n",
    "*Back to [table of contents](#Table-of-Contents)*\n",
    "\n",
    "In exercise 01.1, we will install the SDK and prepare the service key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe9c4aaf566ad9",
   "metadata": {},
   "source": [
    "## Installing the SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd958a5650bacfa",
   "metadata": {},
   "source": [
    "The Data Attribute Recommendation SDK is available from the Python package repository. It can be installed with the standard `pip` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c1429d543918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:17:28.245636Z",
     "start_time": "2025-01-08T06:17:23.467249Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install data-attribute-recommendation-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c3ab2d972f411",
   "metadata": {},
   "source": [
    "*Note: If you are not using a Jupyter notebook, but instead a regular Python development environment, we recommend using a Python virtual environment to set up your development environment. Please see [the dedicated tutorial to learn how to install the SDK inside a Python virtual environment](https://developers.sap.com/tutorials/cp-aibus-dar-sdk-setup.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4cf834dc1bed",
   "metadata": {},
   "source": [
    "## Creating a service instance and key on BTP Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ce6829bca1519",
   "metadata": {},
   "source": [
    "Please log in to your trial account: https://account.hanatrial.ondemand.com/\n",
    "\n",
    "In your global account screen, go to the \"Boosters\" tab:\n",
    "\n",
    "![trial_booster.png](images/trial_booster.png)\n",
    "\n",
    "*Boosters are also available in production. If you are using a production environment, please follow this [tutorial](https://developers.sap.com/tutorials/cp-aibus-dar-booster-free-key.html) to set up account for Data Attribute Recommendation and get service key using either the free or the standard plan*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39337ca3024158db",
   "metadata": {},
   "source": [
    "In the Boosters tab, enter \"Data Attribute Recommendation\" into the search box. Then, select the\n",
    "service tile from the search results: \n",
    "    \n",
    "![trial_locate_dar_booster.png](images/trial_locate_dar_booster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d9a00266a8aaa",
   "metadata": {},
   "source": [
    "The resulting screen shows details of the booster pack. Here, click the \"Start\" button and wait a few seconds.\n",
    "\n",
    "![trial_start_booster.png](images/trial_start_booster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483047cea55c9a44",
   "metadata": {},
   "source": [
    "Once the booster is finished, click the \"Download Service Key\" link to obtain your service key and save it to the disk.\n",
    "\n",
    "![trial_booster_finished.png](images/trial_booster_finished.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec77061f2e5e36",
   "metadata": {},
   "source": [
    "## Loading the service key into your Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b563425998bf1a1",
   "metadata": {},
   "source": [
    "Once you downloaded the service key from the Cockpit, upload it to your notebook environment. The service key must be uploaded to same directory where the `Data_Attribute_Recommendation_AI_API.ipynb` file is stored.\n",
    "\n",
    "When using Jupyterlab, a file browser is visible to the left of the notebook view. Click the upload button here to upload the `default_key.json` file we downloaded earlier from the BTP Cockpit.\n",
    "\n",
    "\n",
    "![service_key_main_jupyter_page.png](images/service_key_main_jupyter_page.png)\n",
    "\n",
    "\n",
    "\n",
    "![service_key_upload.png](images/service_key_upload.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f265e89209b902",
   "metadata": {},
   "source": [
    "Once you click the upload button, a file chooser dialog will open where you can select the `default_key.json`:\n",
    "After the upload finished successfully, you should see the `default_key.json` in the file browser.\n",
    "**Make sure that the file name is `default_key.json`. If your service key file has a different name, this notebook will not work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c5f80afafdbca",
   "metadata": {},
   "source": [
    "The service key contains your credentials to access the service. Please treat this as carefully as you would treat any password. We keep the service key as a separate file outside this notebook to avoid leaking the secret credentials.\n",
    "\n",
    "The service key is a JSON file. We will load this file once and use the credentials throughout this workshop. "
   ]
  },
  {
   "cell_type": "code",
   "id": "8d223b84-7676-499c-b083-17d4e3fbb20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T08:52:15.628448Z",
     "start_time": "2025-02-05T08:52:15.626586Z"
    }
   },
   "source": [
    "# First, set up logging so we can see the actions performed by the SDK behind the scenes\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,stream=sys.stdout)\n",
    "\n",
    "from pprint import pprint  # for better output formatting"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6c0af14ca71568b",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"default_key.json\"):\n",
    "    msg = \"'default_key.json' is not found. Please follow instructions above to create a service key of\"\n",
    "    msg += \" Data Attribute Recommendation. Then, upload it into the same directory where\"\n",
    "    msg += \" this notebook is saved.\"\n",
    "    print(msg)\n",
    "    raise ValueError(msg)\n",
    "\n",
    "with open(\"default_key.json\") as file_handle:\n",
    "    key = file_handle.read()\n",
    "    SERVICE_KEY = json.loads(key)\n",
    "    print(\"Service URL: \")\n",
    "    pprint(SERVICE_KEY[\"url\"])\n",
    "    print(\"Client ID:\")\n",
    "    pprint(SERVICE_KEY[\"uaa\"][\"clientid\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9346f9a7254e018",
   "metadata": {},
   "source": [
    "## Summary Exercise 01.1\n",
    "\n",
    "In exercise 01.1, we have covered the following topics:\n",
    "\n",
    "* How to install the Python SDK for Data Attribute Recommendation\n",
    "* How to obtain a service key for the Data Attribute Recommendation service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab43efa9d9eae4",
   "metadata": {},
   "source": [
    "# Exercise 01.2\n",
    "\n",
    "*Back to [table of contents](#Table-of-Contents)*\n",
    "\n",
    "*To perform this exercise, you need to execute the code in all previous exercises.*\n",
    "\n",
    "In exercise 01.2, we will upload our demo dataset to the service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13525f2ce8973149",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26f67832a0061d",
   "metadata": {},
   "source": [
    "### Obtaining data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb7e11f305c8d3",
   "metadata": {},
   "source": [
    "The dataset we use in this workshop is a CSV file containing scientific paper titles and their topic categories. This dataset is ideal to understand use cases where the labels are independent of one another. What this means is that the presence or absence of one label does not influence the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770f82bcd810185",
   "metadata": {},
   "source": [
    "Let's inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "id": "95f63615ab6662c4",
   "metadata": {},
   "source": [
    "# if you are experiencing an import error here, run the following in a new cell:\n",
    "# ! pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/arxiv.csv\")\n",
    "df.head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91f45b8d26337930",
   "metadata": {},
   "source": [
    "df.tail()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d34c9267d88d5e1c",
   "metadata": {},
   "source": [
    "print()\n",
    "print(f\"Data has {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4d1359482978fdf",
   "metadata": {},
   "source": [
    "The CSV contains the titles of several scientific papers. For each title, the set of topics associated with the title are provided as labels. The following are the labels and their associated full forms.\n",
    "- CSC: Computer Science\n",
    "- STA: Statistics\n",
    "- QFI: Quantitative Finance\n",
    "- QBI: Quantitative Biology\n",
    "- PHY: Physics\n",
    "\n",
    "For example, the instance of the dataset with the title `Contemporary machine learning: a guide for practitioners in the physical sciences` has the following set of labels:\n",
    "- Computer Science\n",
    "- Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67175696516149",
   "metadata": {},
   "source": [
    "We will use the Data Attribute Recommendation service to predict the labels for a given paper based on its **title**. However, you can add other attributes such as length of the paper, number of words, conference name and type to improve the classifier further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8eea87e20d25",
   "metadata": {},
   "source": [
    " ### Initializing DAR AI API client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1b2b0",
   "metadata": {},
   "source": [
    "First, we initialize the DAR AI API Client that we will use troughought the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "70819918378f37eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:07:52.628963Z",
     "start_time": "2025-02-05T09:07:52.626759Z"
    }
   },
   "source": [
    "from sap.aibus.dar.client.aiapi.dar_ai_api_client import DARAIAPIClient\n",
    "\n",
    "url = SERVICE_KEY['url']\n",
    "client_id = SERVICE_KEY['uaa']['clientid']\n",
    "client_secret = SERVICE_KEY['uaa']['clientsecret']\n",
    "auth_url = SERVICE_KEY['uaa']['url']\n",
    "\n",
    "dar_ai_api_client = DARAIAPIClient(\n",
    "    base_url=url + '/model-manager/v2/lm',\n",
    "    auth_url=auth_url + '/oauth/token',\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "735899c888b43209",
   "metadata": {},
   "source": [
    "### Creating the dataset schema"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4375f19e6f1573a",
   "metadata": {},
   "source": [
    "file_path = \"data/schema_arxiv.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    schema = json.load(json_file)\n",
    "\n",
    "pprint(schema)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e0f6844125fe7df",
   "metadata": {},
   "source": [
    "### Uploading data to the service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b33a61c4c461b",
   "metadata": {},
   "source": [
    "We will now upload our dataset and dataset schema files using the DAR AI API Client which we created earlier.\n",
    "\n",
    "The dataset must be a CSV file and fit to the dataset schema. The CSV file can optionally be `gzip` compressed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa9c6836ae90d6",
   "metadata": {},
   "source": [
    "We first have to describe the format of our data by creating a dataset schema. This schema informs the service about the individual column types found in the CSV. We also describe which are the target columns used for training. These columns will be later predicted.\n",
    "\n",
    "The service currently supports three column types: **TEXT**, **CATEGORY** and **NUMBER**. As labels to be predicted, only **CATEGORY** and **NUMBER** are currently supported.\n",
    "\n",
    "For this example, we have prepared the dataset schema already and it can be found in [data/schema_arxiv.json](./data/schema_arxiv.json). We can look at it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "91369a026d2b3bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:07:55.666995Z",
     "start_time": "2025-02-05T09:07:55.458045Z"
    }
   },
   "source": [
    "# Compress file first for a faster upload\n",
    "! gzip -9 -c data/arxiv.csv > data/arxiv.csv.gz"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "431465bd",
   "metadata": {},
   "source": [
    "The dataset and dataset schema files are uploaded using the file_upload_client.put_file() method in the DARAIAPIClient."
   ]
  },
  {
   "cell_type": "code",
   "id": "5780fbea514a65a1",
   "metadata": {},
   "source": [
    "dataset_upload_response = dar_ai_api_client.file_upload_client.put_file(\n",
    "    local_path='data/arxiv.csv.gz',\n",
    "    remote_path='/trial-test/arxiv.csv.gz',\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "dataset_url = dataset_upload_response.json()[\"url\"]\n",
    "print(\"The uploaded dataset URL: \", dataset_url)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f486ab83344f8b9",
   "metadata": {},
   "source": [
    "schema_upload_response = dar_ai_api_client.file_upload_client.put_file(\n",
    "    local_path='data/schema_arxiv.json',\n",
    "    remote_path='/trial-test/schema_arxiv.json',\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "schema_url = schema_upload_response.json()[\"url\"]\n",
    "print(\"The uploaded dataset schema URL: \", schema_url)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b89048db4bc80da9",
   "metadata": {},
   "source": [
    "## Summary Exercise 01.2\n",
    "\n",
    "In exercise 01.2, we have covered the following topics:\n",
    "\n",
    "* How to create a dataset schema\n",
    "* How to upload a dataset and the dataset schema files to the service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85600e14cb2d79",
   "metadata": {},
   "source": [
    "# Exercise 01.3\n",
    "\n",
    "*Back to [table of contents](#Table-of-Contents)*\n",
    "\n",
    "*To perform this exercise, you need to execute the code in all previous exercises.*\n",
    "\n",
    "In exercise 01.3, we will register the artifacts and  train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1258c91026f6c6",
   "metadata": {},
   "source": [
    "## Select the Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80966150f571d5fb",
   "metadata": {},
   "source": [
    "To train a machine learning model, we first need to select the correct scenario. You can refer to the [official documentation on Scenarios](https://help.sap.com/docs/data-attribute-recommendation/data-attribute-recommendation/scenarios?locale=en-US) to learn more. Additional scenarios may be added over time, so check back regularly.\n",
    "\n",
    "We can also query the list of scenarios through the DAR AI API:"
   ]
  },
  {
   "cell_type": "code",
   "id": "25e585b2e8996a80",
   "metadata": {},
   "source": [
    "from ai_api_client_sdk.models.scenario_query_response import ScenarioQueryResponse\n",
    "\n",
    "scenario_query_response: ScenarioQueryResponse = dar_ai_api_client.scenario.query()\n",
    "for scenario in scenario_query_response.resources:\n",
    "     pprint(scenario.__dict__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f7cda4a",
   "metadata": {},
   "source": [
    "In this exercise, we are building a model to predict labels which are independent of one another. The scenario **Generic model template** is correct for this excercise. "
   ]
  },
  {
   "cell_type": "code",
   "id": "319ba206946dbc33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:08:09.526531Z",
     "start_time": "2025-02-05T09:08:09.524942Z"
    }
   },
   "source": [
    "scenario_id = \"ccb99c7c-07c1-45f5-b51b-3e7d8b76eb0c\" # Scenario ID of Generic model template"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "d3e7e8d6880c6d3e",
   "metadata": {},
   "source": [
    "## Artifact Registration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55025873f72fcb9",
   "metadata": {},
   "source": [
    "Before training the model, the uploaded dataset and dataset schema files need to be registered as artifacts."
   ]
  },
  {
   "cell_type": "code",
   "id": "fcd440c783089fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:08:11.333880Z",
     "start_time": "2025-02-05T09:08:11.332060Z"
    }
   },
   "source": [
    "from ai_api_client_sdk.models.artifact_create_response import ArtifactCreateResponse\n",
    "from ai_api_client_sdk.models.artifact import Artifact"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "bb87aa3eea018d4",
   "metadata": {},
   "source": [
    "artifact_response: ArtifactCreateResponse = dar_ai_api_client.artifact.create(\n",
    "    name=\"datasetschema\",\n",
    "    kind=Artifact.Kind.OTHER,\n",
    "    url=schema_url,\n",
    "    scenario_id=scenario_id,\n",
    "    description=\"Trial test dataset schema\"\n",
    ")\n",
    "\n",
    "datasetschema_artifact_id = artifact_response.id\n",
    "print(f\"The artifact (dataset schema) ID is {artifact_response.id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2275902efa3f0f2",
   "metadata": {},
   "source": [
    "artifact_response: ArtifactCreateResponse = dar_ai_api_client.artifact.create(\n",
    "    name=\"dataset\",\n",
    "    kind=Artifact.Kind.DATASET,\n",
    "    url=dataset_url,\n",
    "    scenario_id=scenario_id,\n",
    "    description=\"Trial test dataset\")\n",
    "\n",
    "dataset_artifact_id =  artifact_response.id\n",
    "print(f\"The artifact (dataset) ID is {artifact_response.id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "277b08fc4f05e68d",
   "metadata": {},
   "source": [
    "## Select the Executable\n",
    "Each Scenario comes with multiple Executables to do different tasks, for example a training executable or a deployment executable. You can refer to the [official documentation on Supported Executables](https://help.sap.com/docs/data-attribute-recommendation/data-attribute-recommendation/supported-executables) to learn more. \n",
    "\n",
    "We can also query the list of executables for through the DAR AI API:"
   ]
  },
  {
   "cell_type": "code",
   "id": "6fca496ddb667c01",
   "metadata": {},
   "source": [
    "executables = dar_ai_api_client.executable.query(scenario_id=scenario_id, version_id='3.0')\n",
    "for executable in executables.resources:\n",
    "    pprint(executable.__dict__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "954b9fca96620893",
   "metadata": {},
   "source": [
    "The \"Generic Training Executable\" is selected as the training executable from the list."
   ]
  },
  {
   "cell_type": "code",
   "id": "4c4fbc88b84b7f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:08:18.455482Z",
     "start_time": "2025-02-05T09:08:18.453907Z"
    }
   },
   "source": [
    "training_executable_id = \"40dcde13-ce0f-45cc-aac0-74da78175305\""
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "83e2a9f0b3770008",
   "metadata": {},
   "source": [
    "## Create a Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785690115263a05",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To start the training execution, we need to bring together the information we've assembled so far: the IDs of the dataset artifact and dataset schema artifact, the training executable and the desired scenario. We also have to provide a name for the model.\n",
    "\n",
    "Generally, parameters are added using ParameterBinding objects and input artifacts are added using InputArtifactBinding objects.\n",
    "\n",
    "*Only one model of a given name can exist. If you receive a message stating 'The model name specified is already in use', you either have to remove the training execution that created the model with that name or you have to change the `modelName` in the ParameterBinding below. You can also [clean up the entire service instance](#Cleaning-up-a-service-instance).*"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ee74dd0bcd07f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:08:19.865847Z",
     "start_time": "2025-02-05T09:08:19.864245Z"
    }
   },
   "source": [
    "from ai_api_client_sdk.models.input_artifact_binding import InputArtifactBinding\n",
    "from ai_api_client_sdk.models.parameter_binding import ParameterBinding"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "1d6b2cd2031212e5",
   "metadata": {},
   "source": [
    "# Create input artifact bindings\n",
    "input_artifact_bindings = [\n",
    "    InputArtifactBinding(key=\"dataset\", artifact_id=dataset_artifact_id),\n",
    "    InputArtifactBinding(key=\"datasetSchema\", artifact_id=datasetschema_artifact_id)\n",
    "]\n",
    "\n",
    "# Create parameter bindings\n",
    "parameter_bindings = [\n",
    "    ParameterBinding(key=\"modelName\", value=\"trial_model\")\n",
    "]\n",
    "\n",
    "# Create the Configuration\n",
    "training_configuration = dar_ai_api_client.configuration.create(\n",
    "    name=\"trial_training_config\",\n",
    "    scenario_id=scenario_id,\n",
    "    executable_id=training_executable_id,\n",
    "    input_artifact_bindings=input_artifact_bindings,\n",
    "    parameter_bindings=parameter_bindings,\n",
    ")\n",
    "print(f\"Training Configuration ID: {training_configuration.id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc687822ab63fb92",
   "metadata": {},
   "source": [
    "## Create a Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "id": "30cac3024f79c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:08:23.119900Z",
     "start_time": "2025-02-05T09:08:23.117380Z"
    }
   },
   "source": [
    "from ai_api_client_sdk.models.execution_create_response import ExecutionCreateResponse"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "2e580699d21ae7af",
   "metadata": {},
   "source": [
    "execution_response: ExecutionCreateResponse = dar_ai_api_client.execution.create(configuration_id=training_configuration.id)\n",
    "print(f\"Execution ID: {execution_response.id}, Status: {execution_response.status}, Message: {execution_response.message}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3fe45e0854514cc",
   "metadata": {},
   "source": [
    "## Get Training Status\n",
    "The training execution is now running in the background and we can poll the execution's status until it reaches \"COMPLETED\".\n",
    "The `DARAIAPIClient` provides a `get()` method which could be used to find the current status of the training execution."
   ]
  },
  {
   "cell_type": "code",
   "id": "76f8a20d795da085",
   "metadata": {},
   "source": [
    "from ai_api_client_sdk.models.execution import Execution\n",
    "\n",
    "training_execution: Execution = dar_ai_api_client.execution.get(execution_id=execution_response.id)\n",
    "print(f\"The current status of the Execution {execution_response.id} is {training_execution.status}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f7f79fd",
   "metadata": {},
   "source": [
    "Repeat the above cell execution until the reported status is \"COMPLETED\". Once that is the case, the trained model will be listed as an output artifact of the training execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749a2c875d017c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:30:56.795532Z",
     "start_time": "2025-01-08T06:30:56.793876Z"
    }
   },
   "outputs": [],
   "source": [
    "print(training_execution.output_artifacts[0].url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f546f4132b8dc41",
   "metadata": {},
   "source": [
    "## Summary Exercise 01.3\n",
    "\n",
    "In exercise 01.3, we have covered the following topics:\n",
    "\n",
    "* How to select the appropriate Scenario and Executable\n",
    "* How to register dataset and dataset schema as Artifacts\n",
    "* How to configure a training execution and obtain a model artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f530ec04e412b",
   "metadata": {},
   "source": [
    "# Exercise 01.4\n",
    "\n",
    "*Back to [table of contents](#Table-of-Contents)*\n",
    "\n",
    "*To perform this exercise, you need to execute the code in all previous exercises.*\n",
    "\n",
    "In exercise 01.4, we will deploy the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e288ac",
   "metadata": {},
   "source": [
    "## Select the Executable\n",
    "The training execution has finished and the model is ready to be deployed. By deploying the model, we create a server process in the background on the Data Attribute Recommendation service which will serve inference requests.\n",
    "\n",
    "Just like for the training execution, we need to select the appropriate Executable (this time the \"Deployment Exectubale\" instead of the \"Training Executable\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the Executables\n",
    "executables = dar_ai_api_client.executable.query(scenario_id=scenario_id, version_id='3.0')\n",
    "for executable in executables.resources:\n",
    "    pprint(executable.__dict__)\n",
    "\n",
    "# We select the Deployment Executable\n",
    "deployment_executable_id = \"88d4a864-117c-43df-882a-81b490c1919d\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9c8e4",
   "metadata": {},
   "source": [
    "## Create a Deployment Configuration\n",
    "The deployment configuration is assembled similarly to the training configuration that we created earlier. We use the model artifact from the training execution as an input artifact for the deployment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ecc0c8d9a0e07b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:31:01.869530Z",
     "start_time": "2025-01-08T06:31:01.867114Z"
    }
   },
   "outputs": [],
   "source": [
    "model_artifact_id = training_execution.output_artifacts[0].id\n",
    "\n",
    "input_artifact_bindings = [\n",
    "    InputArtifactBinding(key=\"model\", artifact_id=model_artifact_id)\n",
    "]\n",
    "\n",
    "deployment_configuration = dar_ai_api_client.configuration.create(\n",
    "    name=\"trial_deployment_config\",\n",
    "    scenario_id=scenario_id,\n",
    "    executable_id=deployment_executable_id,\n",
    "    input_artifact_bindings=input_artifact_bindings,\n",
    ")\n",
    "      \n",
    "print(f\"Deployment Configuration ID: {deployment_configuration.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1ef91920f3635",
   "metadata": {},
   "source": [
    "## Create a Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4dfa27825de3f76d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:31:13.281794Z",
     "start_time": "2025-01-08T06:31:13.280094Z"
    }
   },
   "outputs": [],
   "source": [
    "from ai_api_client_sdk.models.deployment_create_response import DeploymentCreateResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d3e707a57663d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:31:14.580734Z",
     "start_time": "2025-01-08T06:31:13.826887Z"
    }
   },
   "outputs": [],
   "source": [
    "deployment_response:  DeploymentCreateResponse = dar_ai_api_client.deployment.create(configuration_id=deployment_configuration.id)\n",
    "print(f\"Deployment ID: {deployment_response.id}, Status: {deployment_response.status}, Message: {deployment_response.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003e4ead36605e5",
   "metadata": {},
   "source": [
    "## Get Deployment Status\n",
    "We can poll the API until the Deployment is in status \"RUNNING\". The `DARAIAPIClient` provides a `get()` method for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77e7bac23190950a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:31:20.172956Z",
     "start_time": "2025-01-08T06:31:20.171422Z"
    }
   },
   "outputs": [],
   "source": [
    "from ai_api_client_sdk.models.deployment import Deployment\n",
    "\n",
    "deployment_execution: Deployment = dar_ai_api_client.deployment.get(deployment_id=deployment_response.id)\n",
    "print(f\"The current status of the Deployment {deployment_response.id} is {deployment_execution.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c51e23",
   "metadata": {},
   "source": [
    "Repeat the above cell execution until the reported status is \"RUNNING\". Once that is the case, we can extract the URL of our deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e7bf7a7683266",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:39:30.188757Z",
     "start_time": "2025-01-08T06:39:30.186824Z"
    }
   },
   "outputs": [],
   "source": [
    "deployment_url = deployment_execution.deployment_url\n",
    "print(deployment_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d44b0b4a6f598e",
   "metadata": {},
   "source": [
    "## Summary Exercise 01.4\n",
    "\n",
    "In exercise 01.3, we have covered the following topics:\n",
    "\n",
    "* How to select the appropriate Executable for a Deployment\n",
    "* How to configure and create a Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443d963a31eb380",
   "metadata": {},
   "source": [
    "# Exercise 01.5\n",
    "\n",
    "*Back to [table of contents](#Table-of-Contents)*\n",
    "\n",
    "*To perform this exercise, you need to execute the code in all previous exercises.*\n",
    "\n",
    "In exercise 01.5, we will predict labels for some unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dc556fcba312dc",
   "metadata": {},
   "source": [
    "With a single inference request, we can send up to 50 objects to the service to predict the labels. The data sent to the service must match the `features` section of the dataset schema created earlier. The `labels` defined inside of the dataset schema will be predicted for each object and returned as a response to the request.\n",
    "\n",
    "In the SDK, the [`InferenceClient.create_inference_request()`](https://data-attribute-recommendation-python-sdk.readthedocs.io/en/latest/api.html#sap.aibus.dar.client.inference_client.InferenceClient.create_inference_request) method handles submission of inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "72f30e19d1f358d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:39:45.827104Z",
     "start_time": "2025-01-08T06:39:45.824740Z"
    }
   },
   "outputs": [],
   "source": [
    "from sap.aibus.dar.client.inference_client import InferenceClient\n",
    "\n",
    "inference_client = InferenceClient.construct_from_credentials(\n",
    "    dar_url=url,\n",
    "    clientid=client_id,\n",
    "    clientsecret=client_secret,\n",
    "    uaa_url=auth_url,\n",
    ")\n",
    "\n",
    "objects_to_be_classified = [\n",
    "    {\n",
    "        \"features\": [\n",
    "            {\"name\": \"title\", \"value\": \"Not even wrong: The spurious link between biodiversity and ecosystem functioning\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "inference_response = inference_client.create_inference_request_with_url(url=deployment_url,objects=objects_to_be_classified)\n",
    "pprint(inference_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0994e04",
   "metadata": {},
   "source": [
    "*Note: For trial accounts, you only have a limited number of objects which you can classify.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb97096",
   "metadata": {},
   "source": [
    "You can also try to come up with your own example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc72d77298e8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:39:53.151390Z",
     "start_time": "2025-01-08T06:39:51.941591Z"
    }
   },
   "outputs": [],
   "source": [
    "my_own_items = [\n",
    "    {\n",
    "        \"features\": [\n",
    "            {\"name\": \"title\", \"value\": \"EDIT THIS\"}        \n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "inference_response = inference_client.create_inference_request_with_url(url=deployment_url,objects=my_own_items)\n",
    "print()\n",
    "print(\"Inference request processed. Response:\")\n",
    "print()\n",
    "pprint(inference_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf489f064ce57c2",
   "metadata": {},
   "source": [
    "In some cases, the predicted category has the special value `nan`. In the `arxiv.csv` dataset, not all records have the full set of categories. Some records only have one label and some having up to three. The model learns this fact from the data and will occasionally suggest that a record should not have a label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d660ed33a732c8",
   "metadata": {},
   "source": [
    "## Summary Exercise 01.5\n",
    "\n",
    "In exercise 01.5, we have covered the following topics:\n",
    "\n",
    "* How to predict labels for some unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b3c7d79b817ae8",
   "metadata": {},
   "source": [
    "# Wrapping up AI API\n",
    "\n",
    "In this workshop, we looked into the following topics:\n",
    "\n",
    "* Installation of the Python SDK for Data Attribute Recommendation \n",
    "* Uploading and registering a dataset and dataset schema to the DAR AI API\n",
    "* Training a model\n",
    "* Deploying the trained model\n",
    "* Predicting labels for unlabelled data\n",
    "\n",
    "Using these tools, we are able to solve the problem of missing Master Data attributes starting from just a CSV file containing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867b3939f786fa6",
   "metadata": {},
   "source": [
    " ## Clean Up Instance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390cd50ffdfc9568",
   "metadata": {},
   "source": [
    "In this notebook, we have created several resources on the Data Attribute Recommendation Service:\n",
    "\n",
    "* Uploaded csv and json files\n",
    "* Training execution and the trained model\n",
    "* Deployment\n",
    "\n",
    "The SDK provides several methods to delete these resources. NOTE: The deletion of artifacts and configurations are not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4c366b9b06257",
   "metadata": {},
   "source": [
    "## Clean the Service Instance \n",
    "In general, if Executions or Deployments are in \"PENDING\" or \"RUNNING\" status, they need to be set to \"STOPPED\" before they can be deleted. This is usually the case for Deployments because their final status is \"RUNNING\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b38d35a955161",
   "metadata": {},
   "source": [
    "### Stop and delete the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "686aad7a85c10cf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:20:43.532742Z",
     "start_time": "2025-01-08T06:20:43.531166Z"
    }
   },
   "outputs": [],
   "source": [
    "from ai_api_client_sdk.models.target_status import TargetStatus\n",
    "\n",
    "deployment_modify_response = dar_ai_api_client.deployment.modify(\n",
    "    deployment_id=deployment_response.id,\n",
    "    target_status=TargetStatus.STOPPED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check deployment status\n",
    "deployment_execution = dar_ai_api_client.deployment.get(deployment_id=deployment_response.id)\n",
    "print(f\"The current status of the Deployment {deployment_response.id} is {deployment_execution.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad1ba4a",
   "metadata": {},
   "source": [
    "Repeat the above cell until the deployment's status is \"STOPPED\", then move on to delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_deletion_response = dar_ai_api_client.deployment.delete(deployment_id=deployment_response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8befd3",
   "metadata": {},
   "source": [
    "### Delete the execution"
   ]
  },
  {
   "cell_type": "code",
   "id": "8993f6d78310751c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:16:36.719750Z",
     "start_time": "2025-02-05T09:16:35.926577Z"
    }
   },
   "source": "execution_deletion_response = dar_ai_api_client.execution.delete(execution_id=execution_response.id)",
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "ec269d3a768485e3",
   "metadata": {},
   "source": [
    "### Delete the uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "45a6b9ec925f56b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T06:40:28.153840Z",
     "start_time": "2025-01-08T06:40:26.778494Z"
    }
   },
   "outputs": [],
   "source": [
    "deletedataset_response = dar_ai_api_client.file_upload_client.delete_file(remote_path='/trial-test/arxiv.csv.gz')\n",
    "deletedatasetschema_response = dar_ai_api_client.file_upload_client.delete_file(remote_path='/trial-test/schema_arxiv.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7f8997e588e7f",
   "metadata": {},
   "source": [
    "## List the Remaining Executions and Deployments"
   ]
  },
  {
   "cell_type": "code",
   "id": "689ff0c6f007dec1",
   "metadata": {},
   "source": [
    "executions = dar_ai_api_client.execution.query()\n",
    "\n",
    "for d in executions.resources:\n",
    "    print(f\"{d}: {d.status} ({d.target_status})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = dar_ai_api_client.deployment.query()\n",
    " \n",
    "for d in deployments.resources:\n",
    "    print(f\"{d}: {d.status} ({d.target_status}). deployment URL: {d.deployment_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
